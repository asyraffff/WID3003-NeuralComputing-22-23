{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvYjyBphKmk/49rrG/He9b"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8b-S667EYyb"
      },
      "source": [
        "##Classifying MNIST handwriting digits with Multi Layer Perceptron(MLP)\n",
        "\n",
        "* In this session, we will create a fully connected MLP with one hidden layer, train and evaluate the network on the MNIST dataset. \n",
        "* The MNIST dataset is small enough that we can use a simple MLP for training( (The MNIST images are relatively small)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1JnsNcWI5N2"
      },
      "source": [
        "##What does our planned MLP looks like? Draw the diagram \n",
        "\n",
        "* How many inputs?\n",
        "* How many hidden layers?\n",
        "* How many neurons in the hidden layers?\n",
        "* How many neurons in the output layer?\n",
        "* What about activation functions?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSCH5t2Q9ft-"
      },
      "source": [
        "##The Main Steps\n",
        "\n",
        "Generally, the main steps for building a Deep Learning Neural Network are as follows. \n",
        "\n",
        "1. Import libraries, seed\n",
        "2. Set data preprocessing (transform), download dataset, split train and test \n",
        "3. Set Dataloaders \n",
        "4. Define the model class\n",
        "5. Set loss function, optimizer and learning rate\n",
        "6. Training : Load the data\n",
        "7. Training : Zero the parameter gradients\n",
        "8. Training : Compute fwd\n",
        "9. Training : Compute loss\n",
        "10. Training : Compute backward,set optimizer(update weights)\n",
        "10. Evaluation of trained model on test dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNFEQyulhw-t"
      },
      "source": [
        "##1. Import libraries, seeding random for reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWLMUg75h1WD"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import numpy as np \n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True  \n",
        "torch.backends.cudnn.benchmark = False #for a small dataset, simple network , this is not really needed\n",
        "np.random.seed(seed)\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6hjWnsMXkcC"
      },
      "source": [
        "##2. Download the MNIST dataset and Data pre-processing\n",
        "\n",
        "* Each PIL image is converted to a PyTorch tensor using transforms.ToTensor()\n",
        "* The 28*28 image data is flattened into a vector using lambda which is a customized transformation. Here lambda returns a new view of the input tensor which is the product of the input tensor's dimension. \n",
        "\n",
        "* We are also splitting the MNIST dataset into training and test dataset\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUuXmfLu2qAC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea890117-527c-4bfd-cec6-8f9f1a986a09"
      },
      "source": [
        "\n",
        "transformCustom = transforms.Compose([\n",
        "                                transforms.ToTensor(), #this convert to tensor\n",
        "                                transforms.Lambda(lambda x:x.view(-1))  #this flatten 28*28 into a 784 vector for each image\n",
        "])\n",
        "\n",
        "\n",
        "train = datasets.MNIST(root='.',train=True,transform=transformCustom, download=True)\n",
        "test = datasets.MNIST(root='.', train=False, transform=transformCustom,download=True)\n",
        "\n",
        "print(len(train.data))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 102851876.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 34550968.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 25885152.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 15718258.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "60000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUt7Ri6UlD6w"
      },
      "source": [
        "#3. DataLoaders\n",
        "\n",
        "* Previously like in the perceptron assignment, we passed data manually. Depends on the implementation, for each iteration sometimes we pass one row of data, or one mini batch of data, or one whole batch of data. And for the case of Stochastic Gradient Descent, we need to randomly shuffle the dataset or randomly pick one sample from the dataset i.e we have to do it manually\n",
        "\n",
        "* In PyTorch, we can use DataLoader class that automatically pass the batches of data fetched from a Dataset object. We can also set if we want the DataLoader to shuffle the data and the size of each batch of data.\n",
        "\n",
        "* Each batch is a tuple containing the images in the first element and the labels in the second"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lj8rAV1rlG6j"
      },
      "source": [
        "#Set DataLoader\n",
        "batchSize = 128  # Rule of thumb is to set to the power of 2. In this case 2^7\n",
        "train_loader = DataLoader(train, batch_size=batchSize,shuffle=True)\n",
        "test_loader = DataLoader(test,batch_size=batchSize, shuffle=False) # no need to shuffle test data\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJeiPHmXEh62"
      },
      "source": [
        "#Task:\n",
        "\n",
        "What is the shape of the train data and train label?\n",
        "\n",
        "How many batches are there? Is the size for each batch the same ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kI7w4INaEzgY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "210d1a9c-f2f2-40d3-ef96-805e4d095bbb"
      },
      "source": [
        "count = 0\n",
        "for xb, yb in train_loader:\n",
        "  #Your code here\n",
        "print(f'There are {count} batches in train_loader')\n",
        "#How many batches are there ?   #128*468+96=60,000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-14fb44b5bc1f>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    print(f'There are {count} batches in train_loader')\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after 'for' statement on line 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXiAgnkOoe6a"
      },
      "source": [
        "#Task: \n",
        "Do the same for test data (test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1qqOt_tmxHg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d1cded4-7fb6-40c4-fae5-26c8d2bf4d8b"
      },
      "source": [
        "count=0\n",
        "for x, y in test_loader:\n",
        "  #Your code here\n",
        "print(f'There are {count} batches in test_loader') \n",
        " #78*128+16=10,000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 79 batches in test_loader\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbiHO3EGRmYV"
      },
      "source": [
        "#Define the Neural Net model/class (MLP with one hidden layer, fully connected)\n",
        "\n",
        "* Define our model in a class that extends nn.Module. \n",
        "* nn.Module subclasses must do a minimum of one thing: implement the forward method which takes a batch of data and performs the forward-pass. \n",
        "\n",
        "* PyTorch's autograd system will take care of computing the gradients of the forward pass for us. In the code below we'll also make use of the constructor of our model to instantiate the hidden and output layers.\n",
        "\n",
        "\n",
        "* The nn.Module class defines a instance variable called `training` that is set to True when the model is being trained and False when it is being evaluated after being trained. \n",
        "\n",
        "* In our model definition we've used a softmax activation function on the output layer to turn the outputs into probability-like values, but have only set this to be enabled when we are not training the model. We've done this because we will use PyTorch's implementation of Cross Entropy Loss (nn.CrossEntropyLoss) during training which **implicitly** adds a softmax before a logarithmic loss.\n",
        "\n",
        "* In our case the softmax isn't actually necessary for model evaluation if we're only interested in the most likely class; the logits (unscaled log probabilities) provided by the final fully connected layer before the softmax can be used directly as the largest logit will correspond to the most likely class.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8BT_CFp5S-H"
      },
      "source": [
        "class MLP(nn.Module):\n",
        "  def __init__(self,input_size, hidden_size,num_classes):\n",
        "    super(MLP,self).__init__()\n",
        "\n",
        "    self.layer1 = nn.Linear(input_size,hidden_size) \n",
        "    self.layer2 = nn.Linear(hidden_size,num_classes)\n",
        "\n",
        "  def forward(self,x): \n",
        "\n",
        "    out = self.layer1(x)\n",
        "    #out = F.sigmoid(out) \n",
        "    out = torch.sigmoid(out)\n",
        "    out = self.layer2(out)\n",
        "    #out = softmax(out) #implicitly added during training by nn.CrossEntropyLoss\n",
        "\n",
        "    if not self.training:\n",
        "      out = F.softmax(out,dim=1)\n",
        "    return out\n",
        " \n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPxwE0FXRvtw"
      },
      "source": [
        "#5 Set loss function, optimizer\n",
        "#6-10 Training\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Training and Evaluating the Model\n",
        "* One of the design decisions of PyTorch is that everything should be explicit so we have full control over our models and the training process. \n",
        "\n",
        "* This means that we actually need to write the model training loop by hand, and perform each of the various operations (perform the forward-pass, compute the loss, perform the backward-pass, and update the weights). \n",
        "\n",
        "* In the code below we'll fit the model to the data over several epochs using batches of 128 images provided by the DataLoader defined previously. \n",
        "\n",
        "* We'll make use of the ADAM optimiser as it broadly tends to work well practically despite its limitations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcBNvprNR7Xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08be13b0-ab93-4329-c7f3-a23db761c31b"
      },
      "source": [
        "\n",
        "model = MLP(784, 784, 10) #input_size,hidden_size,num_classes\n",
        "\n",
        "#5. Set loss function and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss() \n",
        "opt = torch.optim.Adam(model.parameters()) #optimizer, optimization strategy-to escape the local minima and to converge quickly\n",
        "#Rule of thumb for optimizer\n",
        "#1. if you want to keep things simple. use ADAM\n",
        "#2. if you have time, then use SGD, and tune the learning rate/parameters (this is usually done by postgrad students)\n",
        "#3. if you are implementing a paper, use the same strategy as what the authors are using \n",
        "\n",
        "epochSize =3 #obviously this isn't enough\n",
        "\n",
        "for epoch in range(epochSize): #this training part can be made into a function, or defined as function of class MLP\n",
        "  \n",
        "   #model.train() #by default, this is set to true. so not really needed. what is important is model.eval() that we'll see that later\n",
        "   #refer https://pytorch.org/docs/stable/generated/torch.nn.Module.html for more details.\n",
        "\n",
        "\n",
        "  loss = 0\n",
        "  # 6. Load the data \n",
        "  for input_batch, target_batch in train_loader:\n",
        "\n",
        "    #7. Zero the gradients\n",
        "    opt.zero_grad() \n",
        "    \n",
        "    #8. Forward pass\n",
        "    predict_batch = model(input_batch) \n",
        "    \n",
        "    #9. Compute loss\n",
        "    loss_batch = loss_fn(predict_batch,target_batch)  \n",
        "    \n",
        "    #10. Backward pass and update weights\n",
        "    loss_batch.backward() \n",
        "    opt.step()\n",
        "\n",
        "    loss += loss_batch.item() #store the loss\n",
        "    \n",
        "\n",
        "  print(f'Epoch: {epoch+1}  loss: {loss}')    \n",
        "  \n",
        "        \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1  loss: 224.84893715381622\n",
            "Epoch: 2  loss: 110.55411703139544\n",
            "Epoch: 3  loss: 84.45046036690474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkEi-hH2jvmr"
      },
      "source": [
        "#Task: Visualize epoch vs loss/cost and minibatches vs loss/cost\n",
        "\n",
        "\n",
        "* In the above we printed out the total loss at the end of each epoch. With your own code, plot the loss.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZBB8Us98GPm"
      },
      "source": [
        "#plot the loss by epochs, minibatches here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Task: Create a function that evaluates trained model on the training dataset\n",
        "\n",
        "* Compute the overall accuracy of the training set. Note that you need a call to model.eval() - this sets the model into evaluation mode and supresses non-training things (gradients, and things such as dropout being applied/computed)."
      ],
      "metadata": {
        "id": "i8eZt4DYf5mz"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20Z7OkDUiI78"
      },
      "source": [
        "def evaluate_model(data_loader): #or def compute_accuracy():  , use whatever conventions that you like\n",
        "\n",
        "\n",
        "  model.eval() # sets the model in evaluation mode \n",
        "  for input_batch, target_batch in data_loader: #data_loader can take train or test dataset\n",
        "  \n",
        "    #Your code here \n",
        "\n",
        "  print('number of evaluated data')\n",
        "  print(f'number of wrongly predicted label ')\n",
        "\n",
        "\n",
        "  print(f'accuracy')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_Fd3_fa8h2r"
      },
      "source": [
        "#11.Task: Evaluation of trained model on test data\n",
        "\n",
        "* Compute the overall accuracy of the test set. Note that you need a call to model.eval() - this sets the model into evaluation mode and supresses non-training things (gradients, and things such as dropout being applied/computed)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPxdTCqbsqqI"
      },
      "source": [
        "\n",
        "evaluate_model(test_loader)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}